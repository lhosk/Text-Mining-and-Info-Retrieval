{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsZEj38QaOhL"
      },
      "source": [
        "# __Recipe Recommendation__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIFp4nmuaN79"
      },
      "source": [
        "### Project Overview\n",
        "\n",
        "#### Inputs:\n",
        "- **Ingredients that you have**\n",
        "- **Ingredients that you are allergic to**\n",
        "\n",
        "#### Outputs:\n",
        "- **Table of all data**\n",
        "- **List of Information for Best Recipe:**\n",
        "  - Type of Recipe (Food, Drink, Other)\n",
        "  - Difficulty Level (Easy, Medium, Hard, Unknown)\n",
        "  - Title\n",
        "  - Ingredients\n",
        "  - Ingredients Missing *(optional if completed)*\n",
        "  - Full Recipe Steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7szCqI09aNtk"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym5OvYi7aM_4"
      },
      "outputs": [],
      "source": [
        "# Initialize Libraries\n",
        "import ast\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import pipeline\n",
        "\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruxRM2g6aPMG"
      },
      "outputs": [],
      "source": [
        "# To save and read data files from your Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77LBbscOHLiY"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = 'N/A'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69aGhrFaQP-"
      },
      "source": [
        "# Import Recipes and Make Updated Data Frame (updated_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9ljBv7baRkR"
      },
      "outputs": [],
      "source": [
        "# Import the github and kaggle files\n",
        "path = '/content/drive/MyDrive/2024 Spring/Text Mining/Projects/FinalFolder/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivSlbVT5Kk2h"
      },
      "source": [
        "### Kaggle recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpr-GWvXcWpe"
      },
      "outputs": [],
      "source": [
        "# kaggle recipes\n",
        "# https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images\n",
        "kaggle_df = pd.read_csv(path+'kaggle_recipes.csv')\n",
        "kaggle_df.info()\n",
        "kaggle_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTNtyxl4KoOq"
      },
      "source": [
        "### GitHub recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb3zeGjzceVe"
      },
      "outputs": [],
      "source": [
        "# github recipes\n",
        "# https://github.com/cweber/cookbook/blob/master/recipes.csv\n",
        "github_df = pd.read_csv(path + 'github_recipes.csv', engine='python')\n",
        "github_df.info()\n",
        "github_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8PivhtjKtaA"
      },
      "source": [
        "### Updated recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VHy8RQSe1vZ"
      },
      "outputs": [],
      "source": [
        "# Put github text into udpated_df\n",
        "updated_df = github_df[['Title', 'Directions']].copy()\n",
        "ingredient_cols = [col for col in github_df.columns if col.startswith('Ingredient')]\n",
        "\n",
        "ingredients_column = []\n",
        "for i in range(len(github_df)):\n",
        "    ingredients = []\n",
        "    for col in ingredient_cols:\n",
        "        v = github_df.at[i, col]\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            ingredients.append(v.strip())\n",
        "    ingredients_column.append(ingredients)\n",
        "\n",
        "updated_df['Ingredients'] = ingredients_column\n",
        "\n",
        "# Make kaggle into real list\n",
        "kaggle_temp = kaggle_df[['Title', 'Cleaned_Ingredients', 'Instructions']].copy()\n",
        "kaggle_temp = kaggle_temp.rename(columns={\n",
        "    'Cleaned_Ingredients': 'Ingredients',\n",
        "    'Instructions': 'Directions'\n",
        "})\n",
        "kaggle_temp['Ingredients'] = kaggle_temp['Ingredients'].apply(ast.literal_eval)\n",
        "\n",
        "# Add kaggle into udpated_df\n",
        "updated_df = pd.concat([updated_df, kaggle_temp], ignore_index=True)\n",
        "updated_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZzTFh3X4Y7I"
      },
      "source": [
        "### Preprocess the Ingredients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd_e1r8v4cKl"
      },
      "outputs": [],
      "source": [
        "unicode_fracs = r'[\\u00BC-\\u00BE\\u2150-\\u215E]'\n",
        "def my_preprocessor(list_of_strings):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    text: (list(str))\n",
        "\n",
        "  Changes:\n",
        "    Converts text to lowercase\n",
        "    Removed unicode fractions\n",
        "    Removed numbers\n",
        "    Removed punctuation\n",
        "    Removed stop words\n",
        "    Removed common words\n",
        "  \"\"\"\n",
        "\n",
        "  text_processed_list = []\n",
        "\n",
        "  for text in list_of_strings:\n",
        "    # Makes text lowercase\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Remove unicode fractions\n",
        "    text_uni = re.sub(unicode_fracs, '', text_lower)\n",
        "\n",
        "    # Remove measurements\n",
        "    text_meas = re.sub(r'\\d+[\\d\\s\\/\\.\\-]*', '', text_uni)\n",
        "\n",
        "    # Remove numbers\n",
        "    text_num = re.sub(r'\\d+', '', text_meas)\n",
        "\n",
        "    # Split text into words (also gets rid of punctuation)\n",
        "    tokens = RegexpTokenizer(r'\\w+').tokenize(text_num)\n",
        "\n",
        "    # Removes stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stopunct_tokens = []\n",
        "    for token in tokens:\n",
        "      if token not in stop_words:\n",
        "        stopunct_tokens.append(token)\n",
        "\n",
        "    # Combines text\n",
        "    text_processed_0 = ' '.join(stopunct_tokens)\n",
        "\n",
        "    # Removes common words\n",
        "    common_words = [\n",
        "    'additional', 'accompaniment',\n",
        "    'basic', 'bit', 'blend',\n",
        "    'chopped', 'chunk', 'chunks', 'cook', 'cooked', 'crosswise', 'cubed', 'cup', 'cups', 'cut',\n",
        "    'diced', 'divided',\n",
        "    'fine', 'finely',\n",
        "    'g', 'good', 'gram', 'grams',\n",
        "    'half',\n",
        "    'inch',\n",
        "    'kg',\n",
        "    'large', 'lb', 'like',\n",
        "    'medium', 'minute', 'ml',\n",
        "    'optional', 'ounce', 'ounces',\n",
        "    'pan', 'patted', 'pieces', 'plus', 'pound', 'pounds', 'precooked',\n",
        "    'quality', 'quart', 'quartered', 'qt',\n",
        "    'room',\n",
        "    'serving', 'sliced', 'slices', 'size', 'small', 'smooth', 'spoon',\n",
        "    'tablespoon', 'tablespoons', 'taste', 'tbsp', 'teaspoon', 'teaspoons', 'temp', 'temperature', 'thermometer', 'tsp',\n",
        "    'whole'\n",
        "    ]\n",
        "\n",
        "    final_tokens = []\n",
        "    for token in text_processed_0.split():\n",
        "      if token not in common_words:\n",
        "        final_tokens.append(token)\n",
        "\n",
        "    # Make one string again\n",
        "    text_processed = ' '.join(final_tokens)\n",
        "    if text_processed.strip():\n",
        "      text_processed_list.append(text_processed)\n",
        "\n",
        "  return text_processed_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ3e4nR6-LLR"
      },
      "outputs": [],
      "source": [
        "# Make a list of processed ingredients (strings) (will be used later for counting the # of ingredients)\n",
        "updated_df['Ingredients'] = updated_df['Ingredients'].apply(my_preprocessor)\n",
        "\n",
        "# Turn those into strings for embedding\n",
        "updated_df['Ingredients_Text'] = updated_df['Ingredients'].apply(lambda ingredient: ', '.join(ingredient))\n",
        "\n",
        "# Delete rows that have NaN in any of the columns\n",
        "updated_df = updated_df.dropna()\n",
        "\n",
        "updated_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "046Kg7aoREFq"
      },
      "source": [
        "# Predict Recipe Type (Food, Drink, Other)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL7DK3-fLO1Y"
      },
      "outputs": [],
      "source": [
        "updated_df = pd.read_csv(path + 'updated_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TukSDyvRI8V"
      },
      "outputs": [],
      "source": [
        "# If updated_df exists in drive, load it\n",
        "if os.path.exists(path + 'updated_df.csv'):\n",
        "    updated_df = pd.read_csv(path + 'updated_df.csv')\n",
        "else: # Apply classifier and save it\n",
        "\n",
        "  batch_size = 40  # Keep at 40 for token safety\n",
        "\n",
        "  def classify_batch_with_gpt(batch_df):\n",
        "      prompt_intro = f\"\"\"Classify each recipe below as one of the following: food, drink, or other.\n",
        "  If it is a soup, dessert, or cake, classify as other.\n",
        "\n",
        "  Respond with exactly {len(batch_df)} numbered lines like this:\n",
        "  1. food\n",
        "  2. drink\n",
        "  ...\n",
        "\n",
        "  Only return the classification word per line. Do not include titles or directions in your response.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  prompt_body = \"\"\n",
        "  for i, row in enumerate(batch_df.itertuples(), start=1):\n",
        "    prompt_body += f\"{i}) Title: {row.Title}\\nDirections: {row.Directions}\\n\\n\"\n",
        "\n",
        "  full_prompt = prompt_intro + prompt_body\n",
        "\n",
        "  response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "    temperature=0\n",
        "  )\n",
        "\n",
        "  lines = response.choices[0].message.content.strip().splitlines()\n",
        "  cleaned = [line.split(\".\")[-1].strip().lower() for line in lines if \".\" in line]\n",
        "  if len(cleaned) != len(batch_df):\n",
        "    raise ValueError(f\"Mismatched response length: expected {len(batch_df)}, got {len(cleaned)}\")\n",
        "\n",
        "  return cleaned\n",
        "\n",
        "  for start in range(0, len(updated_df), batch_size):\n",
        "    end = min(start + batch_size, len(updated_df))\n",
        "    batch_df = updated_df.iloc[start:end].copy()\n",
        "    try:\n",
        "      classifications = classify_batch_with_gpt(batch_df)\n",
        "      updated_df.loc[start:end-1, 'Recipe_Type'] = classifications\n",
        "      print(f\"Processed {end} rows\")\n",
        "\n",
        "  # updated_df.to_csv(path + 'updated_df.csv', index=False)\n",
        "\n",
        "updated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ti14c1AHmo_"
      },
      "outputs": [],
      "source": [
        "# Delete rows that have NaN (just in case)\n",
        "updated_df = updated_df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ogr-pljiiYL"
      },
      "source": [
        "# Choose ingredients you have, are allergic to, and the recipe type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zoe8pg_ivbo"
      },
      "outputs": [],
      "source": [
        "# have = ['chicken, water, rice, olive oil, onion, garlic, salt, pepper, pasta, beef, and tomatoes, butter, oil, celery, carrot']\n",
        "have = ['bread, butter, cheese']\n",
        "allergic = ['peanut, nut, tree nut, cashew, hazelnut, macaroon, pistachio, almond, coconuts, adhesive']\n",
        "\n",
        "# Choose recipe type ('all', 'food', 'drink', 'other')\n",
        "recipe_type = 'food'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwB_AQRNTf1z"
      },
      "source": [
        "# Start Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0zItfp-Irk"
      },
      "source": [
        "### Make model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHBQhdPSi_mh"
      },
      "outputs": [],
      "source": [
        "# Make model\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2') # does not do well\n",
        "model = SentenceTransformer('paraphrase-mpnet-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y0Ntnl7Tmii"
      },
      "outputs": [],
      "source": [
        "# If embeddings are saved, use them. Othewise, make and save them\n",
        "\n",
        "# All recipe types\n",
        "if recipe_type == 'all':\n",
        "  if os.path.exists(path + 'all_recipe_embedding.npy'):\n",
        "      all_recipe_embedding = np.load(path + 'all_recipe_embedding.npy')\n",
        "  else:\n",
        "    recipe_text = updated_df['Ingredients_Text'].tolist()\n",
        "    all_recipe_embedding = model.encode(recipe_text, batch_size=64, show_progress_bar=True)\n",
        "    np.save(path + 'all_recipe_embedding.npy', all_recipe_embedding)\n",
        "\n",
        "# Food type\n",
        "elif recipe_type == 'food':\n",
        "  if os.path.exists(path + 'food_recipe_embedding.npy'):\n",
        "      food_recipe_embedding = np.load(path + 'food_recipe_embedding.npy')\n",
        "  else:\n",
        "    food_recipe_text = updated_df[updated_df['Recipe_Type'] == 'food']['Ingredients_Text'].tolist()\n",
        "    food_recipe_embedding = model.encode(food_recipe_text, batch_size=64, show_progress_bar=True)\n",
        "    np.save(path + 'food_recipe_embedding.npy', food_recipe_embedding)\n",
        "\n",
        "# Drink type\n",
        "elif recipe_type == 'drink':\n",
        "  if os.path.exists(path + 'drink_recipe_embedding.npy'):\n",
        "      drink_recipe_embedding = np.load(path + 'drink_recipe_embedding.npy')\n",
        "  else:\n",
        "    drink_recipe_text = updated_df[updated_df['Recipe_Type'] == 'drink']['Ingredients_Text'].tolist()\n",
        "    drink_recipe_embedding = model.encode(drink_recipe_text, batch_size=64, show_progress_bar=True)\n",
        "    np.save(path + 'drink_recipe_embedding.npy', drink_recipe_embedding)\n",
        "\n",
        "# Other type\n",
        "elif recipe_type == 'other':\n",
        "  if os.path.exists(path + 'other_recipe_embedding.npy'):\n",
        "      other_recipe_embedding = np.load(path + 'other_recipe_embedding.npy')\n",
        "  else:\n",
        "    other_recipe_text = updated_df[updated_df['Recipe_Type'] == 'other']['Ingredients_Text'].tolist()\n",
        "    other_recipe_embedding = model.encode(other_recipe_text, batch_size=64, show_progress_bar=True)\n",
        "    # Save embedding\n",
        "    np.save(path + 'other_recipe_embedding.npy', other_recipe_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF2z4wCki4aV"
      },
      "outputs": [],
      "source": [
        "# Determine Embedding Type\n",
        "if recipe_type == 'all':\n",
        "    recipe_embedding = all_recipe_embedding\n",
        "    scores_df = updated_df\n",
        "elif recipe_type == 'food':\n",
        "    recipe_embedding = food_recipe_embedding\n",
        "    scores_df = updated_df[updated_df['Recipe_Type'] == 'food'].reset_index(drop=True)\n",
        "elif recipe_type == 'drink':\n",
        "    recipe_embedding = drink_recipe_embedding\n",
        "    scores_df = updated_df[updated_df['Recipe_Type'] == 'drink'].reset_index(drop=True)\n",
        "elif recipe_type == 'other':\n",
        "    recipe_embedding = other_recipe_embedding\n",
        "    scores_df = updated_df[updated_df['Recipe_Type'] == 'other'].reset_index(drop=True)\n",
        "else:\n",
        "    raise ValueError(\"Invalid recipe type. Choose from 'all', 'food', 'drink', 'other'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_N0OWRV-Kc-"
      },
      "source": [
        "### Determine raw and normalized scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsrgMtWRYrwf"
      },
      "outputs": [],
      "source": [
        "# Embed the foods we have\n",
        "have_string = ', '.join(have)\n",
        "have_embedding = model.encode([have_string])\n",
        "\n",
        "# Embed the foods we are allergic to\n",
        "allergic_string = ', '.join(allergic)\n",
        "allergic_embedding = model.encode([allergic_string])\n",
        "\n",
        "# Determine scores for foods we have and are allergic to\n",
        "have_scores = cosine_similarity(have_embedding, recipe_embedding)[0]\n",
        "allergic_scores = cosine_similarity(allergic_embedding, recipe_embedding)[0]\n",
        "\n",
        "# Put scores in scores_df\n",
        "scores_df.loc[:, 'Have_Score_Raw'] = have_scores\n",
        "scores_df.loc[:, 'Allergic_Score_Raw'] = allergic_scores\n",
        "scores_df.loc[:, 'Combined_Score_Raw'] = scores_df['Have_Score_Raw'] - scores_df['Allergic_Score_Raw']\n",
        "\n",
        "# Normalize scores\n",
        "scaler = MinMaxScaler()\n",
        "raw = np.vstack([have_scores, allergic_scores]).T  # shape (n_recipes, 2)\n",
        "normed = scaler.fit_transform(raw)\n",
        "\n",
        "# Put normalized scores in scores_df\n",
        "scores_df.loc[:, 'Have_Score'] = normed[:, 0]\n",
        "scores_df.loc[:, 'Allergic_Score'] = normed[:, 1]\n",
        "scores_df.loc[:, 'Combined_Score'] = scores_df['Have_Score'] - scores_df['Allergic_Score']\n",
        "\n",
        "# Sort by normalized combined score\n",
        "sorted_df = scores_df.sort_values(by='Combined_Score', ascending=False)\n",
        "sorted_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_LPVARE-P3W"
      },
      "source": [
        "### Take away recipes the user is allergic to (Allergic Score > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeJ69JU9dRPX"
      },
      "outputs": [],
      "source": [
        "# Take away raw scores\n",
        "no_allergies_df = sorted_df.drop(columns=['Have_Score_Raw', 'Allergic_Score_Raw', 'Combined_Score_Raw'])\n",
        "no_allergies_df\n",
        "\n",
        "# Take away recipes that most likely have allergies\n",
        "no_allergies_df = no_allergies_df[no_allergies_df['Allergic_Score'] < 0.5]\n",
        "no_allergies_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UdbeFTTPHV1"
      },
      "source": [
        "# Use HuggingFace to see what Ingredients I don't have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTXtvhlyPSLj"
      },
      "source": [
        "### Import Hugging Face and start the chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GpzbrQgPVOt"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install hugchat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_FI0FTNPXdP"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceApi, InferenceClient\n",
        "from hugchat import hugchat\n",
        "from hugchat.login import Login\n",
        "import os\n",
        "\n",
        "Email = 'lhosk'\n",
        "Password = 'NA'\n",
        "\n",
        "# Log in to huggingface and grant authorization to huggingchat\n",
        "sign = Login(Email, Password)\n",
        "cookies = sign.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT1aGfS-84TO"
      },
      "outputs": [],
      "source": [
        "# Start a chatbot\n",
        "chatbot = hugchat.ChatBot(cookies=cookies.get_dict())  # or cookie_path=\"usercookies.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYBsSKzyyiQg"
      },
      "outputs": [],
      "source": [
        "# Start a new conversation\n",
        "conversation_id = chatbot.new_conversation() # To get a new conversation ID\n",
        "chatbot.change_conversation(conversation_id) # So start a new conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp6n8SOCPzxR"
      },
      "source": [
        "### Test HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npM_dMvFPQOn"
      },
      "outputs": [],
      "source": [
        "top3_df = no_allergies_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4qWo0OzP18r"
      },
      "outputs": [],
      "source": [
        "top3_df = top3_df.drop(columns=['Ingredients', 'Allergic_Score', 'Combined_Score'])\n",
        "\n",
        "top3_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwY-9V6QSrtC"
      },
      "outputs": [],
      "source": [
        "for i, row in top3_df.iterrows():\n",
        "  recipe_title = row[\"Title\"]\n",
        "  recipe_directions = row[\"Directions\"]\n",
        "  recipe_ingredients = row[\"Ingredients_Text\"]\n",
        "\n",
        "  prompt = f\"\"\"I have the following ingredients: {', '.join(have)}.\n",
        "  The recipe is {recipe_title}.\n",
        "  These are the directions: {recipe_directions}.\n",
        "  These are the ingredients: {recipe_ingredients}\n",
        "\n",
        "  Also group all cheese together. Like you can generalize a bit and get rid of cooking sprays and stuff\n",
        "\n",
        "  Print: Name of Recipe\n",
        "  Print missing ingredients: You are missing: ingredient, ingredient, ...\n",
        "  Next Line Print: You have (percentage %) of the ingredients.\n",
        "  Next Line Print if there are alternatives.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  response = chatbot.chat(prompt)\n",
        "  print(f\"{response} \\n \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNxmkBXOUtkC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PsZEj38QaOhL",
        "7szCqI09aNtk",
        "u69aGhrFaQP-",
        "ivSlbVT5Kk2h",
        "uTNtyxl4KoOq",
        "v8PivhtjKtaA",
        "bZzTFh3X4Y7I",
        "046Kg7aoREFq",
        "JTXtvhlyPSLj"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
