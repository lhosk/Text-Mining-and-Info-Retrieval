{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ESXGaaeZoRnY",
        "J4BwSDrv2bKq",
        "FmzZcPqD2jJp",
        "_NnTtcxs5Lic",
        "jVZ56SLaC5h9",
        "WhWLJkaAF6JL",
        "19GnxO2zDvph",
        "4YabILSPc-N8",
        "9z5VuB8aEfYO",
        "xPEF2zOOGbEI",
        "EMljcWWw1HN1",
        "pLgdPpdtGQp6",
        "OrcjiMAMUCNG",
        "Ry5yBcUBUZqJ",
        "kARgvO-qgQU1",
        "rnTxxwDXAw_4",
        "-_W8wtV72pD_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization, Making the Preprocessor, and Making the Processed Training File"
      ],
      "metadata": {
        "id": "ESXGaaeZoRnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "mEH9pYGro10O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and read data files from your GoogleDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oyZEDjODoQu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training .csv file\n",
        "old_training_doc_df = pd.read_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/topic_model_train_new.csv')"
      ],
      "metadata": {
        "id": "htzgOdfpoeLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_lemma = spacy.load(\"en_core_web_sm\")\n",
        "def my_preprocessor(text):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    text: (str)\n",
        "\n",
        "  Changes:\n",
        "    Converts text to lowercase\n",
        "    Removed Markdown code\n",
        "    Removed numbers\n",
        "    Removed stop words (english and spanish)\n",
        "    Removed parentheses (and everything inside them)\n",
        "    Removed punctuation\n",
        "    Lemmatizes\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Makes text lowercase\n",
        "  text_lower = text.lower()\n",
        "\n",
        "  # Remove parentheses and anything inside them\n",
        "  text_paren = re.sub(r'\\(.*?\\)', '', text_lower)\n",
        "\n",
        "  # Remove numbers\n",
        "  text_num = re.sub(r'\\d+', '', text_paren)\n",
        "\n",
        "  # Remove markdown / LaTeX code (starts with '\\' or '$' or contain underscores)\n",
        "  text_clean = re.sub(r'[\\$\\\\]\\S+|\\b\\w*_\\w*\\b', '', text_num)\n",
        "\n",
        "  # Split text into words (also gets rid of punctuation)\n",
        "  tokens = RegexpTokenizer(r'\\w+').tokenize(text_clean)\n",
        "\n",
        "  # Define stop words\n",
        "  stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
        "\n",
        "  # Removes stopwords\n",
        "  stopunct_tokens = []\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      stopunct_tokens.append(token)\n",
        "\n",
        "  # Combines text for and processes with SpaCy\n",
        "  text_processed_0 = ' '.join(stopunct_tokens)\n",
        "  text_spacy = spacy_lemma(text_processed_0)\n",
        "\n",
        "  # Lemmatization\n",
        "  pos_tags = {'NOUN', 'ADJ', 'VERB', 'ADV'}\n",
        "  lemma_tokens = []\n",
        "  for token in text_spacy:\n",
        "    if token.pos_ in pos_tags:\n",
        "      lemma_tokens.append(token.lemma_)\n",
        "\n",
        "  common_words = [\n",
        "      'approach',\n",
        "      'consider',\n",
        "      'define',\n",
        "      'different',\n",
        "      'feature',\n",
        "      'first',\n",
        "      'general',\n",
        "      'however',\n",
        "      'known',\n",
        "      'method',\n",
        "      'network',\n",
        "      'number',\n",
        "      'obtain',\n",
        "      'present',\n",
        "      'problem',\n",
        "      'propose',\n",
        "      'provide',\n",
        "      'result'\n",
        "  ]\n",
        "\n",
        "  # Keep words > 4 letters and rid of common words with little meaning\n",
        "  final_tokens = []\n",
        "  for token in lemma_tokens:\n",
        "    if len(token) > 5 and token not in common_words:\n",
        "      final_tokens.append(token)\n",
        "\n",
        "  # Make one string again\n",
        "  text_processed = ' '.join(final_tokens)\n",
        "\n",
        "  return text_processed"
      ],
      "metadata": {
        "id": "zonnOBrwpRkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and save the processed training file (if doesn't exist yet)\n",
        "file_path = '/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/training_processed.csv'\n",
        "if os.path.exists(file_path):\n",
        "  train_df = pd.read_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/training_processed.csv')\n",
        "  print('File Already Exists. Continue.')\n",
        "else:\n",
        "  train_df = old_training_doc_df.copy()\n",
        "  train_df['ABSTRACT'] = train_df['ABSTRACT'].apply(my_preprocessor)\n",
        "  path = '/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/'\n",
        "  train_df.to_csv(f'{path}training_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "EReMpWZSpglE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a histogram of the word count per abstract\n",
        "train_df['ABSTRACT_word_count'] = train_df['ABSTRACT'].apply(lambda x: len(str(x).split(\" \")))\n",
        "plt.hist(train_df['ABSTRACT_word_count'])\n",
        "plt.xlabel(\"Words per Abstract\")\n",
        "plt.ylabel(\"Abstracts\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aZlpbhjQsT4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at two examples of processed vs. not processed\n",
        "for i in range(2): # Use to see first Abstract\n",
        "    abstract = old_training_doc_df['ABSTRACT'].iloc[i]\n",
        "    print(f'Abstract {i+1} (Not Processed):', abstract)\n",
        "    abstract = train_df['ABSTRACT'].iloc[i]\n",
        "    print(f'Abstract {i+1} (Processed):', abstract, '\\n')"
      ],
      "metadata": {
        "id": "yhKO4VFr0kE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Embedding (SBERT - \"allenai-specter\") on the Training Set"
      ],
      "metadata": {
        "id": "J4BwSDrv2bKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "FmzZcPqD2jJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy.spatial import distance\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "L6UxgEkf2euf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making the SBERT Model"
      ],
      "metadata": {
        "id": "_NnTtcxs5Lic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "specter_sbert_model = SentenceTransformer(\"allenai-specter\")"
      ],
      "metadata": {
        "id": "xelvNXFW5Kkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and save the SBERT embeddings (if doesn't exist yet)\n",
        "embeddings_path = '/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/train_sbert_embeddings.npy'\n",
        "if os.path.exists(embeddings_path):\n",
        "  print('(Training) SBERT Embeddings already exist. Continue.')\n",
        "  train_sbert_embeddings = np.load(embeddings_path)\n",
        "else:\n",
        "  train_sbert_embeddings = specter_sbert_model.encode(train_df['ABSTRACT'], batch_size = 32, show_progress_bar=True)\n",
        "  np.save(embeddings_path, train_sbert_embeddings)"
      ],
      "metadata": {
        "id": "DKKSRkRZ5To8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Query"
      ],
      "metadata": {
        "id": "jVZ56SLaC5h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing toy search engine with example query\n",
        "query = 'Artifical Neural Networks Programming Code'\n",
        "query_embedding = specter_sbert_model.encode(query)"
      ],
      "metadata": {
        "id": "i-9zc-X0ZBMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine similarity\n",
        "cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), train_sbert_embeddings)\n",
        "cosine_similarities = cosine_similarities[0]\n",
        "\n",
        "# Dot product\n",
        "dot_products = np.dot(train_sbert_embeddings, query_embedding)\n",
        "\n",
        "# Euclidean distance\n",
        "euclidean_distances = np.array([distance.euclidean(embedding, query_embedding) for embedding in train_sbert_embeddings])\n",
        "\n",
        "# Normalize all scores to get an average\n",
        "normalized_cosine = (cosine_similarities - np.min(cosine_similarities)) / (np.max(cosine_similarities) - np.min(cosine_similarities))\n",
        "normalized_dot = (dot_products - np.min(dot_products)) / (np.max(dot_products) - np.min(dot_products))\n",
        "normalized_euclidean = 1 - (euclidean_distances - np.min(euclidean_distances)) / (np.max(euclidean_distances) - np.min(euclidean_distances))\n",
        "\n",
        "# Add scores to DataFrame\n",
        "old_training_doc_df['Cosine Similarity'] = cosine_similarities\n",
        "old_training_doc_df['Dot Product'] = dot_products\n",
        "old_training_doc_df['Euclidean Distance'] = euclidean_distances\n",
        "old_training_doc_df['Geometric Mean'] = (normalized_cosine * normalized_dot * normalized_euclidean)**(1/3)\n",
        "\n",
        "# Sort and display top results by geometric mean\n",
        "top_10_results = old_training_doc_df.sort_values(by='Geometric Mean', ascending=False).head(10)\n",
        "top_10_results"
      ],
      "metadata": {
        "id": "a2HKxJm9ZwDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Test Data Set"
      ],
      "metadata": {
        "id": "WhWLJkaAF6JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and save the processed training file (if doesn't exist yet)\n",
        "file_path = '/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/test_processed.csv'\n",
        "if os.path.exists(file_path):\n",
        "  test_df = pd.read_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/test_processed.csv')\n",
        "  print('File Already Exists. Continue.')\n",
        "  old_test_df = pd.read_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/topic_model_test_new.csv').copy()\n",
        "else:\n",
        "  old_test_df = pd.read_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/topic_model_test_new.csv').copy()\n",
        "  test_df['ABSTRACT'] = old_test_df['ABSTRACT'].apply(my_preprocessor)\n",
        "  path = '/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/'\n",
        "  test_df.to_csv(f'{path}test_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "iM6_vv07FJ5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a Histogram of the word count per abstract\n",
        "test_df['ABSTRACT_word_count'] = test_df['ABSTRACT'].apply(lambda x: len(str(x).split(\" \")))\n",
        "plt.hist(test_df['ABSTRACT_word_count'])\n",
        "plt.xlabel(\"Words per Abstract\")\n",
        "plt.ylabel(\"Abstracts\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-g2wcyjvNGeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at two examples of processed vs. not processed\n",
        "for i in range(2): # Use to see first Abstract\n",
        "    abstract = old_test_df['ABSTRACT'].iloc[i]\n",
        "    print(f'Abstract {i+1} (Not Processed):', abstract)\n",
        "    abstract = test_df['ABSTRACT'].iloc[i]\n",
        "    print(f'Abstract {i+1} (Processed):', abstract, '\\n')"
      ],
      "metadata": {
        "id": "vL5q5ja2NbK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Embedding (SBERT - \"allenai-specter\" and SBERT - \"all-MiniLM-L6-v2\" ) on Testing Sets"
      ],
      "metadata": {
        "id": "19GnxO2zDvph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Find Top Document and Rank"
      ],
      "metadata": {
        "id": "4YabILSPc-N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_doc(query, model, embeddings, doc_df, true_id):\n",
        "\n",
        "  # Decide on model\n",
        "  if model == 'specter':\n",
        "    query_embedding = specter_sbert_model.encode(query)\n",
        "  elif model == 'minilm':\n",
        "    query_embedding = minilm_sbert_model.encode(query)\n",
        "  else:\n",
        "    print('Enter \"specter\" for allenai-specter and \"minilm\" for all-MiniLM-L6-v2')\n",
        "\n",
        "  # Cosine similarity\n",
        "  cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), embeddings)\n",
        "  cosine_similarities = cosine_similarities[0]\n",
        "\n",
        "  # Dot product\n",
        "  dot_products = np.dot(embeddings, query_embedding)\n",
        "\n",
        "  # Euclidean distance\n",
        "  euclidean_distances = np.array([distance.euclidean(embedding, query_embedding) for embedding in embeddings])\n",
        "\n",
        "  # Normalize all scores to get an average\n",
        "  normalized_cosine = (cosine_similarities - np.min(cosine_similarities)) / (np.max(cosine_similarities) - np.min(cosine_similarities))\n",
        "  normalized_dot = (dot_products - np.min(dot_products)) / (np.max(dot_products) - np.min(dot_products))\n",
        "  normalized_euclidean = 1 - (euclidean_distances - np.min(euclidean_distances)) / (np.max(euclidean_distances) - np.min(euclidean_distances))\n",
        "  doc_df['Geometric Mean'] = (normalized_cosine * normalized_dot * normalized_euclidean)**(1/3)\n",
        "\n",
        "  # Sort by geometric mean\n",
        "  sorted_df = doc_df.sort_values(by='Geometric Mean', ascending=False)\n",
        "\n",
        "  # Get top doc ID\n",
        "  top_doc_id = sorted_df.iloc[0]['ID']\n",
        "\n",
        "  # Determine rank of original topic\n",
        "  rank_doc_id = np.where(sorted_df['ID'] == true_id)[0][0] + 1\n",
        "\n",
        "  return top_doc_id, rank_doc_id"
      ],
      "metadata": {
        "id": "kLKBdOPXU6hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"all-MiniLM-L6-v2\" Model"
      ],
      "metadata": {
        "id": "9z5VuB8aEfYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the model\n",
        "minilm_sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "nGH-c3zZD045"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and save the SBERT embeddings (if doesn't exist yet)\n",
        "embeddings_path = '/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/minilm_test_sbert_embeddings.npy'\n",
        "if os.path.exists(embeddings_path):\n",
        "  print('MiniLM (test) SBERT Embeddings already exist. Continue.')\n",
        "  minilm_sbert_embeddings = np.load(embeddings_path)\n",
        "else:\n",
        "  minilm_sbert_embeddings = minilm_sbert_model.encode(test_df['ABSTRACT'], batch_size = 32, show_progress_bar=True)\n",
        "  np.save(embeddings_path, minilm_sbert_embeddings)"
      ],
      "metadata": {
        "id": "Mb7iPvkhEnwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ID's so it doesn't have to be run again\n",
        "if 'MiniLM Predicted ID' not in test_df.columns or 'MiniLM Rank' not in test_df.columns:\n",
        "  minilm_ids = []\n",
        "  minilm_ranks = []\n",
        "\n",
        "  # Place expected ID and true rank in column\n",
        "  for title in range(test_df.shape[0]):\n",
        "    test_id = test_df['ID'].iloc[title]\n",
        "    query = test_df['TITLE'].iloc[title]\n",
        "    query_embedding = minilm_sbert_model.encode(query)\n",
        "    minilm_id_placeholder, minilm_rank_placeholder = find_top_doc(query, 'minilm', minilm_sbert_embeddings, test_df, test_id)\n",
        "    minilm_ids.append(minilm_id_placeholder)\n",
        "    minilm_ranks.append(minilm_rank_placeholder)\n",
        "\n",
        "\n",
        "  test_df['MiniLM Predicted ID'] = minilm_ids\n",
        "  test_df['MiniLM Rank'] = minilm_ranks\n",
        "  test_df.to_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/test_processed.csv', index=False)\n",
        "else:\n",
        "  print('Predicted MiniLM ID and rank columns exist.')"
      ],
      "metadata": {
        "id": "DxT0PB6M1mE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"allenai-specter\" Model"
      ],
      "metadata": {
        "id": "xPEF2zOOGbEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the model\n",
        "specter_sbert_model = SentenceTransformer(\"allenai-specter\")"
      ],
      "metadata": {
        "id": "slnuUlPKMNSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and save the SBERT embeddings (if doesn't exist yet)\n",
        "embeddings_path = '/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/specter_test_sbert_embeddings.npy'\n",
        "if os.path.exists(embeddings_path):\n",
        "  print('Specter (test) SBERT Embeddings already exist. Continue.')\n",
        "  specter_sbert_embeddings = np.load(embeddings_path)\n",
        "else:\n",
        "  specter_sbert_embeddings = specter_sbert_model.encode(test_df['ABSTRACT'], batch_size = 32, show_progress_bar=True)\n",
        "  np.save(embeddings_path, specter_sbert_embeddings)"
      ],
      "metadata": {
        "id": "lfjStlvoMXrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ID's so it doesn't have to be run again\n",
        "if 'Specter Predicted ID' not in test_df.columns or 'Specter Rank' not in test_df.columns:\n",
        "  specter_ids = []\n",
        "  specter_ranks = []\n",
        "\n",
        "  # Place expected ID in column\n",
        "  for title in range(test_df.shape[0]):\n",
        "    test_id = test_df['ID'].iloc[title]\n",
        "    query = test_df['TITLE'].iloc[title]\n",
        "    query_embedding = specter_sbert_model.encode(query)\n",
        "    specter_id_placeholder, specter_rank_placeholder = find_top_doc(query, 'specter', specter_sbert_embeddings, test_df, test_id)\n",
        "    specter_ids.append(specter_id_placeholder)\n",
        "    specter_ranks.append(specter_rank_placeholder)\n",
        "\n",
        "  test_df['Specter Predicted ID'] = specter_ids\n",
        "  test_df['Specter Rank'] = specter_ranks\n",
        "  test_df.to_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/test_processed.csv', index=False)\n",
        "else:\n",
        "  print('Predicted Specter ID and rank columns exist.')"
      ],
      "metadata": {
        "id": "yJqLiuzIQapE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine Average Recall@1 and MRR From Saved Data, Delete Geometric Mean Column and Show Full Table"
      ],
      "metadata": {
        "id": "EMljcWWw1HN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine Average Recall@1 and MRR\n",
        "specter_recall_count = 0\n",
        "specter_mrr_sum = 0\n",
        "minilm_recall_count = 0\n",
        "minilm_mrr_sum = 0\n",
        "for row in range(len(test_df)):\n",
        "  if test_df['ID'].iloc[row] == test_df['Specter Predicted ID'].iloc[row]:\n",
        "    specter_recall_count += 1\n",
        "  specter_mrr_sum += 1/(test_df['Specter Rank'].iloc[row])\n",
        "  if test_df['ID'].iloc[row] == test_df['MiniLM Predicted ID'].iloc[row]:\n",
        "    minilm_recall_count += 1\n",
        "  minilm_mrr_sum += 1/(test_df['MiniLM Rank'].iloc[row])\n",
        "\n",
        "specter_ave_recall = specter_recall_count / test_df.shape[0]\n",
        "specter_mrr = specter_mrr_sum / test_df.shape[0]\n",
        "minilm_ave_recall = minilm_recall_count / test_df.shape[0]\n",
        "minilm_mrr = minilm_mrr_sum / test_df.shape[0]\n",
        "\n",
        "print(f'Specter Average Recall@1: {specter_ave_recall:.4f}')\n",
        "print(f'Specter MRR: {specter_mrr:.4f}')\n",
        "print(f'Minilm Average Recall@1: {minilm_ave_recall:.4f}')\n",
        "print(f'Minilm MRR: {minilm_mrr:.4f}')"
      ],
      "metadata": {
        "id": "i88bxy9CdbLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete Geometric Mean Column and save file\n",
        "if 'Geometric Mean' in test_df.columns:\n",
        "  test_df = test_df.drop(['Geometric Mean'], axis=1)\n",
        "test_df.to_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/test_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "7jgOn4ZBTl6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF and TF on Testing Sets"
      ],
      "metadata": {
        "id": "pLgdPpdtGQp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "OrcjiMAMUCNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "u3r5nnHCUIMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "Ry5yBcUBUZqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill NaN values with an empty string\n",
        "train_df['ABSTRACT'] = train_df['ABSTRACT'].fillna('')\n",
        "test_df['ABSTRACT'] = test_df['ABSTRACT'].fillna('')\n",
        "\n",
        "# Make vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    sublinear_tf=True,\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        ")\n",
        "\n",
        "# Fit to training data\n",
        "tfidf_vectorizer.fit(train_df['ABSTRACT'])\n",
        "\n",
        "# Transform on test data\n",
        "tfidf_test = tfidf_vectorizer.transform(test_df['ABSTRACT'])\n",
        "\n",
        "# Transform test titles (queries)\n",
        "tfidf_queries = tfidf_vectorizer.transform(test_df['TITLE'])"
      ],
      "metadata": {
        "id": "9Tn3JNlPGVP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill NaN values with# Save ID's so it doesn't have to be run again\n",
        "if 'TFIDF Predicted ID' not in test_df.columns or 'TFIDF Rank' not in test_df.columns:\n",
        "  tfidf_ids = []\n",
        "  tfidf_rank = []\n",
        "  # Place expected ID and rank of actual ID in column\n",
        "  for title in range(test_df.shape[0]):\n",
        "  # for title in range(5):\n",
        "    test_id = test_df['ID'].iloc[title]\n",
        "    query = tfidf_queries[title]\n",
        "    sim_scores = cosine_similarity(query, tfidf_test).flatten()\n",
        "    best_index = sim_scores.argmax()\n",
        "    predicted_id = test_df['ID'].iloc[best_index]\n",
        "\n",
        "    sorted_indices = sim_scores.argsort()[::-1]\n",
        "    sorted_ids = test_df['ID'].iloc[sorted_indices].tolist()\n",
        "    rank = sorted_ids.index(test_id) + 1  # 1-based rank\n",
        "\n",
        "    tfidf_ids.append(predicted_id)\n",
        "    tfidf_rank.append(rank)\n",
        "\n",
        "  test_df['TFIDF Predicted ID'] = tfidf_ids\n",
        "  test_df['TFIDF Rank'] = tfidf_rank\n",
        "  test_df.to_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/test_processed.csv', index=False)\n",
        "else:\n",
        "  print('Predicted TFIDF ID and Rank Columns exist.')"
      ],
      "metadata": {
        "id": "DkIJrYEHWhj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF"
      ],
      "metadata": {
        "id": "kARgvO-qgQU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill NaN values with an empty string\n",
        "train_df['ABSTRACT'] = train_df['ABSTRACT'].fillna('')\n",
        "test_df['ABSTRACT'] = test_df['ABSTRACT'].fillna('')\n",
        "\n",
        "# Make vectorizer\n",
        "tf_vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    sublinear_tf=True,\n",
        "    use_idf=False,\n",
        "    norm=None,\n",
        ")\n",
        "\n",
        "# Fit to training data\n",
        "tf_vectorizer.fit(train_df['ABSTRACT'])\n",
        "\n",
        "# Transform on test data\n",
        "tf_test = tf_vectorizer.transform(test_df['ABSTRACT'])\n",
        "\n",
        "# Transform test titles (queries)\n",
        "tf_queries = tf_vectorizer.transform(test_df['TITLE'])"
      ],
      "metadata": {
        "id": "sXojGJ63gQU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ID's so it doesn't have to be run again\n",
        "if 'TF Predicted ID' not in test_df.columns or 'TF Rank' not in test_df.columns:\n",
        "  tf_ids = []\n",
        "  tf_rank = []\n",
        "  # Place expected ID and rank of actual ID in column\n",
        "  for title in range(test_df.shape[0]):\n",
        "  # for title in range(5):\n",
        "    test_id = test_df['ID'].iloc[title]\n",
        "    query = tf_queries[title]\n",
        "    sim_scores = cosine_similarity(query, tf_test).flatten()\n",
        "    best_index = sim_scores.argmax()\n",
        "    predicted_id = test_df['ID'].iloc[best_index]\n",
        "\n",
        "    sorted_indices = sim_scores.argsort()[::-1]\n",
        "    sorted_ids = test_df['ID'].iloc[sorted_indices].tolist()\n",
        "    rank = sorted_ids.index(test_id) + 1  # 1-based rank\n",
        "\n",
        "    tf_ids.append(predicted_id)\n",
        "    tf_rank.append(rank)\n",
        "\n",
        "  test_df['TF Predicted ID'] = tf_ids\n",
        "  test_df['TF Rank'] = tf_rank\n",
        "  test_df.to_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project3/test_processed.csv', index=False)\n",
        "else:\n",
        "  print('Predicted TF ID and Rank Columns exist.')"
      ],
      "metadata": {
        "id": "QEH8ZsMOgQU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine Average Recall@1 and MRR From Saved Data"
      ],
      "metadata": {
        "id": "rnTxxwDXAw_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine Average Recall@1 and MRR\n",
        "tf_recall_count = 0\n",
        "tf_mrr_sum = 0\n",
        "tfidf_recall_count = 0\n",
        "tfidf_mrr_sum = 0\n",
        "for row in range(len(test_df)):\n",
        "  if test_df['ID'].iloc[row] == test_df['TF Predicted ID'].iloc[row]:\n",
        "    tf_recall_count += 1\n",
        "  tf_mrr_sum += 1/(test_df['TF Rank'].iloc[row])\n",
        "  if test_df['ID'].iloc[row] == test_df['TF Predicted ID'].iloc[row]:\n",
        "    tfidf_recall_count += 1\n",
        "  tfidf_mrr_sum += 1/(test_df['TFIDF Rank'].iloc[row])\n",
        "\n",
        "tf_ave_recall = tf_recall_count / test_df.shape[0]\n",
        "tf_mrr = tf_mrr_sum / test_df.shape[0]\n",
        "tfidf_ave_recall = tfidf_recall_count / test_df.shape[0]\n",
        "tfidf_mrr = tfidf_mrr_sum / test_df.shape[0]\n",
        "\n",
        "print(f'TF Average Recall@1: {tf_ave_recall:.4f}')\n",
        "print(f'TF MRR: {tf_mrr:.4f}')\n",
        "print(f'TFIDF Average Recall@1: {tfidf_ave_recall:.4f}')\n",
        "print(f'TFIDF MRR: {tfidf_mrr:.4f}')"
      ],
      "metadata": {
        "id": "I9qOWK-_yTi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Table and Results"
      ],
      "metadata": {
        "id": "-_W8wtV72pD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'TF Average Recall@1: {tf_ave_recall:.4f}')\n",
        "print(f'TF MRR: {tf_mrr:.4f}')\n",
        "print(f'TFIDF Average Recall@1: {tfidf_ave_recall:.4f}')\n",
        "print(f'TFIDF MRR: {tfidf_mrr:.4f}')\n",
        "print(f'Specter Average Recall@1: {specter_ave_recall:.4f}')\n",
        "print(f'Specter MRR: {specter_mrr:.4f}')\n",
        "print(f'MiniLM Average Recall@1: {minilm_ave_recall:.4f}')\n",
        "print(f'MiniLM MRR: {minilm_mrr:.4f}')"
      ],
      "metadata": {
        "id": "06XI3Q7O2nza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "bxw-sQcR2TuP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}