{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFsdhmgq2I-H"
      },
      "source": [
        "# Initialization and Testing File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lYrHE6WyViQ"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CRDfQNO2xpe"
      },
      "outputs": [],
      "source": [
        "# Import new libraries\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import string\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHuOnf8U3E2h"
      },
      "outputs": [],
      "source": [
        "# Save and read data files from your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1zs3zGD4EyL"
      },
      "outputs": [],
      "source": [
        "# To load the example data file, which is a set of abstracts of academic articles;\n",
        "doc_df = pd.read_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Projects/Project2/topic_model_train_wo_label.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1t19_hZyZBp"
      },
      "source": [
        "### Testing Data File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoBWBmFI6wXB"
      },
      "outputs": [],
      "source": [
        "# Check the first 3 rows and add another column with the number of words per Abstract\n",
        "doc_df['ABSTRACT_word_count'] = doc_df['ABSTRACT'].apply(lambda x: len(str(x).split(\" \")))\n",
        "doc_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTutbefC8RJ6"
      },
      "outputs": [],
      "source": [
        "# Show a Histogram of the word count per abstract\n",
        "plt.hist(doc_df['ABSTRACT_word_count'])\n",
        "plt.xlabel(\"Words per Abstract\")\n",
        "plt.ylabel(\"Abstracts\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYbH_onZ_App"
      },
      "source": [
        "# Making A Preprocessor (For LDA. May have to be changed for sentence embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBoqpu76_jRB"
      },
      "outputs": [],
      "source": [
        "# By looking at some of the abstracts, we can decide if there are certain patterns that we should take out with the preprocessor\n",
        "\n",
        "# for i in range(12): # Use to see first 12 Abstracts\n",
        "for i in range(1): # Use to see first Abstract\n",
        "    abstract = doc_df['ABSTRACT'].iloc[i]\n",
        "    print(f'Abstract {i+1}:')\n",
        "    print(abstract, \"\\n\" + \"-\"*80, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEXKK5bz_FIo"
      },
      "outputs": [],
      "source": [
        "spacy_lemma = spacy.load(\"en_core_web_sm\")\n",
        "def my_preprocessor(text):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    text: (str)\n",
        "\n",
        "  Changes:\n",
        "    Converts text to lowercase\n",
        "    Removed Markdown code\n",
        "    Removed numbers\n",
        "    Removed stop words (english and spanish)\n",
        "    Removed parentheses (and everything inside them)\n",
        "    Removed punctuation\n",
        "    Lemmatizes\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Makes text lowercase\n",
        "  text_lower = text.lower()\n",
        "\n",
        "  # adds flag in texts as period for sentence embedding\n",
        "  # period is replaced with periodflag\n",
        "  # if sentence_embedding:\n",
        "    # text_lower = text.replace('. ', 'periodflag ')\n",
        "\n",
        "  # Remove parentheses and anything inside them\n",
        "  text_paren = re.sub(r'\\(.*?\\)', '', text_lower)\n",
        "\n",
        "  # Remove numbers\n",
        "  text_num = re.sub(r'\\d+', '', text_paren)\n",
        "\n",
        "  # Remove markdown / LaTeX code (starts with '\\' or '$' or contain underscores)\n",
        "  text_clean = re.sub(r'[\\$\\\\]\\S+|\\b\\w*_\\w*\\b', '', text_num)\n",
        "\n",
        "  # Split text into words (also gets rid of punctuation)\n",
        "  tokens = RegexpTokenizer(r'\\w+').tokenize(text_clean)\n",
        "\n",
        "  # Define stop words\n",
        "  stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
        "\n",
        "  # Removes stopwords\n",
        "  stopunct_tokens = []\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      stopunct_tokens.append(token)\n",
        "\n",
        "  # Combines text for and processes with SpaCy\n",
        "  text_processed_0 = ' '.join(stopunct_tokens)\n",
        "  text_spacy = spacy_lemma(text_processed_0)\n",
        "\n",
        "  # Lemmatization\n",
        "  pos_tags = {'NOUN', 'ADJ', 'VERB', 'ADV'}\n",
        "  lemma_tokens = []\n",
        "  for token in text_spacy:\n",
        "    if token.pos_ in pos_tags:\n",
        "      lemma_tokens.append(token.lemma_)\n",
        "\n",
        "  common_words = [\n",
        "      'approach',\n",
        "      'consider',\n",
        "      'define',\n",
        "      'different',\n",
        "      'feature',\n",
        "      'first',\n",
        "      'general',\n",
        "      'however',\n",
        "      'known',\n",
        "      'method',\n",
        "      'network',\n",
        "      'number',\n",
        "      'obtain',\n",
        "      'present',\n",
        "      'problem',\n",
        "      'propose',\n",
        "      'provide',\n",
        "      'result'\n",
        "  ]\n",
        "\n",
        "  # Keep words > 4 letters and rid of common words with little meaning\n",
        "  final_tokens = []\n",
        "  for token in lemma_tokens:\n",
        "    if len(token) > 5 and token not in common_words:\n",
        "    # if len(token) > 5:\n",
        "      final_tokens.append(token)\n",
        "\n",
        "  # Make one string again\n",
        "  text_processed = ' '.join(final_tokens)\n",
        "\n",
        "  return text_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHQ2_-GcJnrQ"
      },
      "outputs": [],
      "source": [
        "# Test the first abstract\n",
        "abstract = doc_df['ABSTRACT'].iloc[1]\n",
        "print(my_preprocessor(abstract))\n",
        "print(abstract)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tpextwu2Mz3"
      },
      "source": [
        "# LDA Topic Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0QoMZ2bJlo6"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KKrYFkEy3-a"
      },
      "outputs": [],
      "source": [
        "# Import new libraries\n",
        "!pip install pyLDAvis\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis.lda_model as sklearnvis\n",
        "import seaborn as sns\n",
        "from gensim.models import CoherenceModel\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTxTgRrMd0Xq"
      },
      "source": [
        "### LDA Function from SKLearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhLLzsA8ylWr"
      },
      "source": [
        "###### Vectorize text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_eC6NNl2MA-"
      },
      "outputs": [],
      "source": [
        "# Use TF-IDF vectorizer to turn abstracts into vectors (may take 6+ minutes)\n",
        "my_vectorizer = TfidfVectorizer(preprocessor=my_preprocessor, max_features = 7500)\n",
        "abstract_vectorized = my_vectorizer.fit_transform(doc_df['ABSTRACT'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5KpCRBIy2HO"
      },
      "outputs": [],
      "source": [
        "# Define the LDA Model\n",
        "# After running 10+ topics, using 4 seems realistic because it always separates the data\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components= 4, # Number of topics\n",
        "                                    doc_topic_prior = None, # Default is 1/n_documents\n",
        "                                    topic_word_prior = None, # Default is 1/n_documents\n",
        "                                    learning_method='batch',  # 'batch' runs slower, but generalizes. 'online' updates iteratively\n",
        "                                    random_state= 10,\n",
        "                                    max_iter=10) # The number of epoches for the training (how many times you wlll go through the entire corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVodk01H0zyL"
      },
      "outputs": [],
      "source": [
        "# Fit LDA Model to TF-IDF Vectors (will take 1+ minutes)\n",
        "lda_top=lda_model.fit_transform(abstract_vectorized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePPkh0XR8HQX"
      },
      "source": [
        "##### Explore output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WvTSPCT8LNl"
      },
      "outputs": [],
      "source": [
        "# Print out the top 10 word tokens in each topic\n",
        "# The output (tokens and their relevance for each topic) of the LDA model can be accessed through either or lad_top or lda_model.components_\n",
        "vocab = my_vectorizer.get_feature_names_out()\n",
        "for i, comp in enumerate(lda_model.components_):\n",
        "    terms_comp = zip(vocab, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(\"Topic \"+str(i+1)+\": \")\n",
        "    for t in sorted_terms:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBPbCMUv8jA1"
      },
      "outputs": [],
      "source": [
        "# Visualize the results using TSNE (May take 2+ minutes)\n",
        "TSNE_model = TSNE(n_components=2, verbose=1, random_state=10, angle=.99, init='pca')\n",
        "tsne_lda = TSNE_model.fit_transform(lda_top)\n",
        "TSNE_df = pd.DataFrame(tsne_lda, columns=['TSNE1', 'TSNE2'])\n",
        "TSNE_df['topic'] = np.argmax(lda_top, axis=1)\n",
        "sns.scatterplot(x=\"TSNE1\", y=\"TSNE2\", hue=\"topic\", data=TSNE_df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47hj4Ns6JW-p"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis_data = sklearnvis.prepare(lda_model, abstract_vectorized, my_vectorizer)\n",
        "pyLDAvis.display(vis_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ9AeiV-RyTG"
      },
      "source": [
        "### LDA with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbwdr3reeMfK"
      },
      "source": [
        "##### Vectorize Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_HwJzSwX7Ri"
      },
      "outputs": [],
      "source": [
        "id2word = corpora.Dictionary([[word] for word in my_vectorizer.get_feature_names_out()])\n",
        "\n",
        "# Converts preprocessed text to Gensim's corpus format\n",
        "def gensim_prep(preprocessed_texts):\n",
        "    tokens = [text.split() for text in preprocessed_texts]\n",
        "    corpus = [id2word.doc2bow(token) for token in tokens]\n",
        "    return id2word, corpus, preprocessed_texts, tokens\n",
        "\n",
        "preprocessed_texts = [\" \".join(text.split()) for text in doc_df['ABSTRACT']]\n",
        "# id2word, corpus, doc_processed, doc_tokens = gensim_prep(preprocessed_texts)\n",
        "id2word, corpus, doc_preprocessed, doc_tokens = gensim_prep(preprocessed_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmGkKb8neXpg"
      },
      "outputs": [],
      "source": [
        "# Build Model (May take 2+ minutes)\n",
        "Num_of_Topics = 4\n",
        "lda_model_gensim = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=Num_of_Topics,\n",
        "    random_state=10,\n",
        "    update_every=1,\n",
        "    chunksize=250,\n",
        "    passes=30,\n",
        "    iterations = 40,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    minimum_probability=0.0001,\n",
        "    per_word_topics=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FTzGLFGeglm"
      },
      "source": [
        "##### Explore Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuR8n8PzeazA"
      },
      "outputs": [],
      "source": [
        "# Print the topics with their top 10 words\n",
        "topics_gensim = lda_model_gensim.print_topics(num_words=10)\n",
        "for topic in topics_gensim:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2wRVqKRfS2G"
      },
      "outputs": [],
      "source": [
        "# Compute coherence score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model_gensim, texts=doc_tokens, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda*100, '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVmlzLtQgzQu"
      },
      "outputs": [],
      "source": [
        "# Visualize LDA\n",
        "pyLDAvis.enable_notebook()\n",
        "vis_data = gensimvis.prepare(lda_model_gensim, corpus, id2word)\n",
        "pyLDAvis.display(vis_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkDaiLNX2lgg"
      },
      "source": [
        "# Sentence Embedding + Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckwMieSvpJUo"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2lKEYgIpLTt"
      },
      "outputs": [],
      "source": [
        "# Import new libraries\n",
        "!pip install -U sentence-transformers\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Doc2Vec, Hierarchical Clustering via SKLearn, and Output via TSNE"
      ],
      "metadata": {
        "id": "SHxdpbdrvNiU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdSE_7hbS3yP"
      },
      "outputs": [],
      "source": [
        "# Process all the text (May take 6+ minutes)\n",
        "doc_processed = [my_preprocessor(text) for text in doc_df['ABSTRACT']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (May take 2+ minutes)\n",
        "tagged_docs = [TaggedDocument(words=word_tokenize(doc), tags=[str(i)]) for i, doc in enumerate(doc_processed)]\n",
        "\n",
        "# Make and train the model\n",
        "model = Doc2Vec(vector_size=40,\n",
        "                min_count=3, # filter out the infrequent tokens, whose term frequency is lower than min_count\n",
        "                epochs=30)\n",
        "\n",
        "\n",
        "model.build_vocab(tagged_docs)\n",
        "model.train(tagged_docs,\n",
        "            total_examples=model.corpus_count,\n",
        "            epochs=model.epochs)"
      ],
      "metadata": {
        "id": "SwdRtrdCrz1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get document vectors (May take 2+ minutes)\n",
        "doc_vectors = [model.infer_vector(word_tokenize(doc)) for doc in doc_processed]\n",
        "\n",
        "# Cluster the documents\n",
        "num_of_clusters = 4\n",
        "\n",
        "my_clustering = AgglomerativeClustering(n_clusters=num_of_clusters,  linkage='ward')\n",
        "my_clustering.fit_predict(doc_vectors)\n",
        "cluster_assignment = my_clustering.labels_"
      ],
      "metadata": {
        "id": "gXgOYpuZvlT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Results\n",
        "results_df = pd.DataFrame({'ABSTRACT': doc_df['ABSTRACT']})\n",
        "\n",
        "cluster_assignment_series = pd.Series(cluster_assignment, name='Cluster')\n",
        "results_df = pd.DataFrame({\n",
        "    'ABSTRACT': doc_df['ABSTRACT'],\n",
        "    'PROCESSED_TEXT': doc_processed,\n",
        "    'Cluster': cluster_assignment\n",
        "})\n",
        "results_df.head()\n"
      ],
      "metadata": {
        "id": "MO04JUDGfGsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Dendrogram (May take 2+ minutes)\n",
        "sch.dendrogram(sch.linkage(doc_vectors, method='ward'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PtkSGK-c1NRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vectors into numpy array for tsne (May take 4+ minutes)\n",
        "embeddings = np.array(doc_vectors)\n",
        "TSNE_model = TSNE(n_components=2, verbose=1, random_state=10, angle=.85, init='pca', perplexity=30)\n",
        "reduced_embeddings = TSNE_model.fit_transform(embeddings)\n",
        "\n",
        "# Display Results\n",
        "TSNE_df = pd.DataFrame(reduced_embeddings, columns=['TSNE1', 'TSNE2'])\n",
        "TSNE_df['topic'] = results_df['Cluster']\n",
        "sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1], hue=results_df['Cluster'], data=TSNE_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "INi3EGs3iR6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SBERT (all-MiniLM-L6-v2), Hierarchical Clustering SKLearn, and Output via Dendrogram and TSNE"
      ],
      "metadata": {
        "id": "Ish9tOmozfkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sbert_model = SentenceTransformer(\"allenai-specter\")\n",
        "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "9rPd3g_szhj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# May take 16+ min\n",
        "# Use sbert model to embed, using doc_processed from Doc2Vec test\n",
        "embeddings = sbert_model.encode(doc_processed, batch_size = 128, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "6dX__klTUBpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Dendrogram (May take 2+ minutes)\n",
        "sch.dendrogram(sch.linkage(embeddings, method='ward'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_RU7ctCyYhLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# May take 2+ minutes\n",
        "embeddings = StandardScaler().fit_transform(embeddings)\n",
        "num_of_clusters = 4\n",
        "my_clustering = AgglomerativeClustering(n_clusters=num_of_clusters,  linkage='ward')\n",
        "my_clustering.fit_predict(embeddings)\n",
        "cluster_assignment = my_clustering.labels_"
      ],
      "metadata": {
        "id": "Y2JWGQapeGw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame({'ABSTRACT': doc_df['ABSTRACT']})\n",
        "\n",
        "cluster_assignment_series = pd.Series(cluster_assignment, name='Cluster')\n",
        "results_df = pd.DataFrame({\n",
        "    'ABSTRACT': doc_df['ABSTRACT'],\n",
        "    'PROCESSED_TEXT': doc_processed,\n",
        "    'Cluster': cluster_assignment\n",
        "})\n",
        "results_df.head()"
      ],
      "metadata": {
        "id": "orVqGnAOfbEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vectors into numpy array for tsne (May take up to 3+ minutes)\n",
        "embeddings = np.array(embeddings)\n",
        "TSNE_model = TSNE(n_components=2, verbose=1, random_state=10, angle=.85, init='pca', perplexity=30)\n",
        "reduced_embeddings = TSNE_model.fit_transform(embeddings)\n",
        "\n",
        "# Display Results\n",
        "TSNE_df = pd.DataFrame(reduced_embeddings, columns=['TSNE1', 'TSNE2'])\n",
        "TSNE_df['topic'] = results_df['Cluster']\n",
        "sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1], hue=results_df['Cluster'], data=TSNE_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KbnNAk6UgGWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CFsdhmgq2I-H",
        "_lYrHE6WyViQ",
        "f1t19_hZyZBp",
        "MYbH_onZ_App",
        "4Tpextwu2Mz3",
        "s0QoMZ2bJlo6",
        "OTxTgRrMd0Xq",
        "yhLLzsA8ylWr",
        "ePPkh0XR8HQX",
        "GZ9AeiV-RyTG",
        "fbwdr3reeMfK",
        "2FTzGLFGeglm",
        "ckwMieSvpJUo",
        "SHxdpbdrvNiU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}